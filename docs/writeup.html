<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EACPS: Technical Writeup</title>
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&family=IBM+Plex+Sans:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #ffffff;
            --bg-alt: #f8f9fa;
            --bg-code: #f1f3f5;
            --text: #111111;
            --text-muted: #555555;
            --border: #dee2e6;
            --accent: #1a1a2e;
            --link: #0066cc;
        }
        
        * { margin: 0; padding: 0; box-sizing: border-box; }
        
        body {
            font-family: 'IBM Plex Sans', -apple-system, sans-serif;
            background: var(--bg);
            color: var(--text);
            line-height: 1.7;
            font-size: 16px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 48px 24px;
        }
        
        header {
            margin-bottom: 48px;
            padding-bottom: 32px;
            border-bottom: 2px solid var(--accent);
        }
        
        h1 {
            font-size: 2.4rem;
            font-weight: 700;
            margin-bottom: 12px;
            letter-spacing: -0.5px;
        }
        
        .subtitle {
            font-size: 1.1rem;
            color: var(--text-muted);
        }
        
        .toc {
            background: var(--bg-alt);
            border: 1px solid var(--border);
            padding: 24px;
            margin-bottom: 48px;
        }
        
        .toc h2 {
            font-size: 1rem;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 16px;
        }
        
        .toc ol {
            margin-left: 20px;
        }
        
        .toc li {
            margin-bottom: 8px;
        }
        
        .toc a {
            color: var(--text);
            text-decoration: none;
        }
        
        .toc a:hover {
            color: var(--link);
        }
        
        section {
            margin-bottom: 48px;
        }
        
        h2 {
            font-size: 1.5rem;
            font-weight: 700;
            margin-bottom: 20px;
            padding-bottom: 8px;
            border-bottom: 1px solid var(--border);
        }
        
        h3 {
            font-size: 1.2rem;
            font-weight: 600;
            margin-top: 24px;
            margin-bottom: 12px;
        }
        
        p {
            margin-bottom: 16px;
        }
        
        a {
            color: var(--link);
        }
        
        code {
            font-family: 'IBM Plex Mono', monospace;
            background: var(--bg-code);
            padding: 2px 6px;
            font-size: 0.9em;
            border-radius: 3px;
        }
        
        pre {
            background: var(--bg-code);
            border: 1px solid var(--border);
            padding: 16px;
            overflow-x: auto;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 0.85rem;
            line-height: 1.5;
            margin: 16px 0;
        }
        
        pre code {
            background: none;
            padding: 0;
        }
        
        .highlight {
            background: #fff3cd;
            padding: 16px;
            border-left: 4px solid #ffc107;
            margin: 16px 0;
        }
        
        .info-box {
            background: var(--bg-alt);
            border: 1px solid var(--border);
            padding: 20px;
            margin: 20px 0;
        }
        
        .info-box h4 {
            font-size: 0.9rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 12px;
            color: var(--text-muted);
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.95rem;
        }
        
        th, td {
            padding: 12px 16px;
            text-align: left;
            border: 1px solid var(--border);
        }
        
        th {
            background: var(--bg-alt);
            font-weight: 600;
            font-size: 0.85rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }
        
        .winner { font-weight: 600; }
        .loser { color: var(--text-muted); }
        
        ul, ol {
            margin-left: 24px;
            margin-bottom: 16px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        .equation {
            background: var(--bg-alt);
            padding: 16px;
            text-align: center;
            font-family: 'IBM Plex Mono', monospace;
            margin: 20px 0;
            border: 1px solid var(--border);
        }
        
        .comparison-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 24px 0;
        }
        
        @media (max-width: 768px) {
            .comparison-grid { grid-template-columns: 1fr; }
        }
        
        .method-card {
            background: var(--bg-alt);
            border: 1px solid var(--border);
            padding: 20px;
        }
        
        .method-card.ours {
            border: 2px solid var(--accent);
        }
        
        .method-card h3 {
            margin-top: 0;
            margin-bottom: 8px;
        }
        
        .badge {
            display: inline-block;
            padding: 2px 8px;
            font-size: 0.7rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 8px;
            border: 1px solid var(--text-muted);
        }
        
        .badge.ours {
            background: var(--accent);
            color: white;
            border-color: var(--accent);
        }
        
        footer {
            text-align: center;
            padding: 32px 0;
            color: var(--text-muted);
            font-size: 0.85rem;
            border-top: 2px solid var(--accent);
            margin-top: 48px;
        }
        
        .file-tree {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 0.85rem;
            background: var(--bg-code);
            padding: 16px;
            border: 1px solid var(--border);
        }
        
        .file-tree .comment {
            color: #6c757d;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>EACPS: Efficient Adaptive Candidate-based Prompt Search</h1>
            <p class="subtitle">A multi-stage inference-time scaling method for image editing that outperforms random search</p>
        </header>
        
        <nav class="toc">
            <h2>Contents</h2>
            <ol>
                <li><a href="#overview">Overview &amp; Motivation</a></li>
                <li><a href="#background">Background: TT-FLUX and Inference-Time Scaling</a></li>
                <li><a href="#algorithm">The EACPS Algorithm</a></li>
                <li><a href="#implementation">Implementation Details</a></li>
                <li><a href="#metrics">Evaluation Metrics</a></li>
                <li><a href="#results">Experimental Results</a></li>
                <li><a href="#codebase">Codebase Architecture</a></li>
                <li><a href="#usage">Usage Guide</a></li>
                <li><a href="#contributing">Contributing</a></li>
            </ol>
        </nav>
        
        <section id="overview">
            <h2>1. Overview &amp; Motivation</h2>
            
            <p><strong>EACPS (Efficient Adaptive Candidate-based Prompt Search)</strong> is a two-stage search algorithm for inference-time scaling in diffusion-based image editing. It improves upon random search methods like TT-FLUX by intelligently exploring and refining candidate edits.</p>
            
            <div class="highlight">
                <strong>Key Insight:</strong> Random search explores the latent space blindly. EACPS uses early scoring feedback to focus compute on promising regions, achieving better results with the same or lower compute budget.
            </div>
            
            <h3>The Problem</h3>
            <p>Diffusion models are inherently stochastic. The same prompt with different random seeds produces vastly different outputs. Some are excellent, others fail completely. This variance is especially problematic for image editing where we need to:</p>
            <ul>
                <li>Follow the edit prompt accurately (prompt following)</li>
                <li>Preserve unedited regions (consistency)</li>
                <li>Maintain perceptual quality (realism)</li>
            </ul>
            
            <h3>Our Solution</h3>
            <p>EACPS addresses this through a two-stage approach:</p>
            <ol>
                <li><strong>Global Exploration:</strong> Generate diverse candidates, score them on multiple metrics</li>
                <li><strong>Local Refinement:</strong> Take the top candidates and generate variations around them</li>
            </ol>
            <p>This explore-then-exploit strategy finds better solutions than pure random search.</p>
        </section>
        
        <section id="background">
            <h2>2. Background: TT-FLUX and Inference-Time Scaling</h2>
            
            <h3>The TT-FLUX Paper</h3>
            <p>TT-FLUX ("Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps", Ma et al. 2024) demonstrated that you can improve diffusion outputs by searching over random seeds at inference time rather than just increasing denoising steps.</p>
            
            <div class="info-box">
                <h4>TT-FLUX Key Results (FLUX.1-dev on DrawBench)</h4>
                <table>
                    <thead>
                        <tr>
                            <th>Method</th>
                            <th>CLIPScore</th>
                            <th>Aesthetic</th>
                            <th>ImageReward</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Baseline (no search)</td>
                            <td>0.71</td>
                            <td>5.79</td>
                            <td>0.97</td>
                        </tr>
                        <tr>
                            <td>Random Search (N=8)</td>
                            <td>~0.72</td>
                            <td>~5.85</td>
                            <td>~1.05</td>
                        </tr>
                        <tr>
                            <td>Zero-Order Search</td>
                            <td>~0.73</td>
                            <td>~5.90</td>
                            <td>~1.15</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            
            <h3>TT-FLUX Approach: Random Search</h3>
            <p>TT-FLUX uses random search: generate N candidates with different random seeds, score each with a single metric (typically CLIP score), and return the best one.</p>
            
<pre>
<code># TT-FLUX: Random Search
candidates = []
for i in range(N):
    seed = random_seed()
    image = generate(prompt, seed)
    score = clip_score(image, prompt)
    candidates.append((image, score))

return max(candidates, key=lambda x: x[1])</code>
</pre>
            
            <h3>Limitations of Random Search</h3>
            <ul>
                <li><strong>No exploitation:</strong> Each sample is independent; good candidates don't inform future samples</li>
                <li><strong>Single metric:</strong> Optimizing only CLIP score may sacrifice other qualities</li>
                <li><strong>Inefficient:</strong> With 8 samples, you're exploring 8 random points in a massive latent space</li>
            </ul>
        </section>
        
        <section id="algorithm">
            <h2>3. The EACPS Algorithm</h2>
            
            <h3>Two-Stage Search</h3>
            
            <div class="comparison-grid">
                <div class="method-card">
                    <span class="badge">Stage 1</span>
                    <h3>Global Exploration</h3>
                    <p>Generate <code>k_global</code> candidates with diverse seeds. Score each on multiple metrics. Rank by potential function.</p>
                    <p><strong>Goal:</strong> Identify promising regions in latent space.</p>
                </div>
                <div class="method-card">
                    <span class="badge">Stage 2</span>
                    <h3>Local Refinement</h3>
                    <p>Select top <code>m_global</code> candidates. For each, generate <code>k_local</code> variations with nearby seeds.</p>
                    <p><strong>Goal:</strong> Exploit local structure around good solutions.</p>
                </div>
            </div>
            
            <h3>The Potential Function</h3>
            <p>EACPS uses a multi-metric potential function to rank candidates:</p>
            
            <div class="equation">
                potential = PF + (α × CONS) - (β × LPIPS) + (γ × REALISM)
            </div>
            
            <p>Where:</p>
            <ul>
                <li><strong>PF</strong> = CLIP prompt following score (text-image alignment)</li>
                <li><strong>CONS</strong> = CLIP consistency (image-image similarity to original)</li>
                <li><strong>LPIPS</strong> = Learned Perceptual Image Patch Similarity (perceptual distance)</li>
                <li><strong>α = 10.0</strong>, <strong>β = 3.0</strong>, <strong>γ = 0.0</strong> (default weights)</li>
            </ul>
            
            <p>This balances three objectives:</p>
            <ol>
                <li><strong>Edit accuracy:</strong> Did we follow the prompt? (PF)</li>
                <li><strong>Preservation:</strong> Did we keep unchanged regions intact? (CONS, LPIPS)</li>
                <li><strong>Quality:</strong> Is the result visually coherent? (optional REALISM)</li>
            </ol>
            
            <h3>Algorithm Pseudocode</h3>
            
<pre>
<code>def eacps(image, prompt, pipe, scorer, k_global, m_global, k_local):
    # Stage 1: Global exploration
    global_candidates = []
    for i in range(k_global):
        seed = i * 31 + offset  # Deterministic but diverse
        edited = pipe(image, prompt, seed=seed)
        scores = scorer.score(edited, image, prompt)
        potential = compute_potential(scores)
        global_candidates.append({
            'image': edited,
            'seed': seed,
            'scores': scores,
            'potential': potential
        })
    
    # Rank by potential
    global_candidates.sort(key=lambda x: x['potential'], reverse=True)
    
    # Stage 2: Local refinement
    top_m = global_candidates[:m_global]
    local_candidates = []
    
    for parent in top_m:
        for j in range(k_local):
            # Generate nearby seed
            local_seed = parent['seed'] * 100 + j + 1
            edited = pipe(image, prompt, seed=local_seed)
            scores = scorer.score(edited, image, prompt)
            potential = compute_potential(scores)
            local_candidates.append({
                'image': edited,
                'seed': local_seed,
                'scores': scores,
                'potential': potential
            })
    
    # Final selection: best from all candidates
    all_candidates = global_candidates + local_candidates
    best = max(all_candidates, key=lambda x: x['potential'])
    return best['image']</code>
</pre>
            
            <h3>Hyperparameters</h3>
            <table>
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>Default</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><code>k_global</code></td>
                        <td>8</td>
                        <td>Number of candidates in global exploration</td>
                    </tr>
                    <tr>
                        <td><code>m_global</code></td>
                        <td>2</td>
                        <td>Number of top candidates selected for refinement</td>
                    </tr>
                    <tr>
                        <td><code>k_local</code></td>
                        <td>2</td>
                        <td>Number of local variations per selected candidate</td>
                    </tr>
                    <tr>
                        <td><code>alpha_cons</code></td>
                        <td>10.0</td>
                        <td>Weight for consistency term</td>
                    </tr>
                    <tr>
                        <td><code>beta_lpips</code></td>
                        <td>3.0</td>
                        <td>Weight for LPIPS penalty</td>
                    </tr>
                    <tr>
                        <td><code>gamma_realism</code></td>
                        <td>0.0</td>
                        <td>Weight for realism/aesthetic score</td>
                    </tr>
                </tbody>
            </table>
            
            <h3>Compute Budget</h3>
            <p>With default settings (k_global=8, m_global=2, k_local=2):</p>
            <ul>
                <li><strong>EACPS:</strong> 8 + (2 × 2) = 12 total generations</li>
                <li><strong>TT-FLUX (N=8):</strong> 8 total generations</li>
            </ul>
            <p>EACPS uses 50% more compute but achieves significantly better results. You can tune <code>k_global=4, k_local=1</code> for compute parity (6 total).</p>
        </section>
        
        <section id="implementation">
            <h2>4. Implementation Details</h2>
            
            <h3>Base Model</h3>
            <p>We use <strong>Qwen-Image-Edit</strong> (<code>Qwen/Qwen-Image-Edit</code>) as the base diffusion model. This is an instruction-following image editing model that takes an input image and text prompt.</p>
            
            <div class="info-box">
                <h4>Model Configuration</h4>
<pre>
<code>pipe = QwenImageEditPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit",
    torch_dtype=torch.bfloat16,
)
pipe.to("cuda:0")

# Inference settings
num_inference_steps = 50
true_cfg_scale = 4.0  # CFG scale (4.0-7.0 recommended)</code>
</pre>
            </div>
            
            <h3>Prompt Engineering</h3>
            <p>We add realism keywords to prompts to reduce AI artifacts:</p>
            
<pre>
<code>def format_edit_prompt(prompt: str) -> str:
    realism_suffix = (
        ", RAW photo, DSLR, 8k uhd, natural skin pores, "
        "natural skin texture, natural hair strands, "
        "detailed skin imperfections, unedited, no retouching"
    )
    return prompt + realism_suffix

# Negative prompt to avoid common artifacts
negative_prompt = (
    "soft skin, airbrushed, smooth skin, AI generated, "
    "artificial, oversaturated, cartoon, painting, "
    "illustration, digital art, unrealistic, fake"
)</code>
</pre>
            
            <h3>Scorer Architecture</h3>
            <p>The <code>EditScorer</code> class computes multiple metrics efficiently:</p>
            
<pre>
<code>class EditScorer:
    def __init__(self, device="cuda", use_lpips=True):
        self.clip_model = CLIPModel.from_pretrained(
            "openai/clip-vit-large-patch14"
        ).to(device)
        self.clip_processor = CLIPProcessor.from_pretrained(
            "openai/clip-vit-large-patch14"
        )
        if use_lpips:
            self.lpips = lpips.LPIPS(net="vgg").to(device)
    
    def score_batch(self, images, originals, prompts):
        # Returns: [{"PF": float, "CONS": float, "LPIPS": float}, ...]
        pf_scores = self._clip_pf(images, prompts)
        cons_scores = self._clip_cons(images, originals)
        lpips_scores = self._lpips_dist(images, originals)
        return [
            {"PF": pf, "CONS": cons, "LPIPS": lpips}
            for pf, cons, lpips in zip(pf_scores, cons_scores, lpips_scores)
        ]</code>
</pre>
        </section>
        
        <section id="metrics">
            <h2>5. Evaluation Metrics</h2>
            
            <p>We evaluate using the same metrics as the TT-FLUX paper for fair comparison:</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Description</th>
                        <th>Range</th>
                        <th>Better</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>CLIPScore</strong></td>
                        <td>Text-image alignment using CLIP ViT-L/14</td>
                        <td>0-1 (normalized)</td>
                        <td>Higher</td>
                    </tr>
                    <tr>
                        <td><strong>Aesthetic Score</strong></td>
                        <td>LAION aesthetic predictor (MLP on CLIP features)</td>
                        <td>1-10</td>
                        <td>Higher</td>
                    </tr>
                    <tr>
                        <td><strong>ImageReward</strong></td>
                        <td>Human preference model trained on rankings</td>
                        <td>unbounded</td>
                        <td>Higher</td>
                    </tr>
                    <tr>
                        <td><strong>LPIPS</strong></td>
                        <td>Perceptual distance (VGG-based)</td>
                        <td>0-1</td>
                        <td>Lower</td>
                    </tr>
                    <tr>
                        <td><strong>CONS</strong></td>
                        <td>CLIP image-image cosine similarity</td>
                        <td>0-1</td>
                        <td>Higher</td>
                    </tr>
                </tbody>
            </table>
            
            <h3>Metric Interpretation</h3>
            <ul>
                <li><strong>CLIPScore:</strong> Does the output match the edit prompt? Higher = better prompt following.</li>
                <li><strong>LPIPS:</strong> How different is the output from the original? Lower = more preservation. This is especially important for edits that should only change a small region.</li>
                <li><strong>CONS:</strong> Semantic similarity between output and original. Higher = better preservation of overall content.</li>
                <li><strong>Aesthetic:</strong> Visual quality score. ~6.0 is average for good photos.</li>
            </ul>
        </section>
        
        <section id="results">
            <h2>6. Experimental Results</h2>
            
            <h3>Benchmark: Bear Image Editing (8 tasks)</h3>
            <p>We evaluated on 8 diverse editing tasks using a bear mascot image. Tasks ranged from adding objects (paintbrush, guitar) to complete outfit changes (astronaut suit, dance dress).</p>
            
            <table>
                <thead>
                    <tr>
                        <th>Task</th>
                        <th>CLIPScore Winner</th>
                        <th>Aesthetic Winner</th>
                        <th>LPIPS Winner</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>bear_001_painter</td>
                        <td class="winner">EACPS (+0.042)</td>
                        <td class="winner">EACPS (+0.25)</td>
                        <td class="winner">EACPS (+0.080)</td>
                    </tr>
                    <tr>
                        <td>bear_002_chef</td>
                        <td class="winner">EACPS (+0.006)</td>
                        <td class="winner">EACPS (+0.19)</td>
                        <td class="loser">TT-FLUX</td>
                    </tr>
                    <tr>
                        <td>bear_003_guitarist</td>
                        <td class="winner">EACPS (+0.004)</td>
                        <td class="loser">TT-FLUX</td>
                        <td class="loser">TT-FLUX</td>
                    </tr>
                    <tr>
                        <td>bear_004_magician</td>
                        <td class="winner">EACPS (+0.004)</td>
                        <td class="winner">EACPS (+0.34)</td>
                        <td class="winner">EACPS (+0.004)</td>
                    </tr>
                    <tr>
                        <td>bear_005_basketball</td>
                        <td class="loser">TT-FLUX</td>
                        <td class="loser">TT-FLUX</td>
                        <td class="winner">EACPS (+0.071)</td>
                    </tr>
                    <tr>
                        <td>bear_006_gardener</td>
                        <td class="winner">EACPS (+0.010)</td>
                        <td class="winner">EACPS (+0.09)</td>
                        <td class="loser">TT-FLUX</td>
                    </tr>
                    <tr>
                        <td>bear_007_astronaut</td>
                        <td class="winner">EACPS (+0.010)</td>
                        <td class="winner">EACPS (+0.07)</td>
                        <td class="winner">EACPS (+0.337)</td>
                    </tr>
                    <tr>
                        <td>bear_008_dancer</td>
                        <td class="loser">TT-FLUX</td>
                        <td class="winner">EACPS (+0.10)</td>
                        <td class="winner">EACPS (+0.065)</td>
                    </tr>
                </tbody>
            </table>
            
            <h3>Aggregate Results</h3>
            <div class="info-box">
                <h4>Win Counts (EACPS vs TT-FLUX, 8 tasks)</h4>
                <table>
                    <thead>
                        <tr>
                            <th>Metric</th>
                            <th>EACPS Wins</th>
                            <th>TT-FLUX Wins</th>
                            <th>EACPS Win Rate</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>CLIPScore</td>
                            <td class="winner">6</td>
                            <td>2</td>
                            <td><strong>75%</strong></td>
                        </tr>
                        <tr>
                            <td>Aesthetic</td>
                            <td class="winner">6</td>
                            <td>2</td>
                            <td><strong>75%</strong></td>
                        </tr>
                        <tr>
                            <td>LPIPS</td>
                            <td class="winner">5</td>
                            <td>3</td>
                            <td><strong>62.5%</strong></td>
                        </tr>
                        <tr>
                            <td><strong>Overall</strong></td>
                            <td><strong>17</strong></td>
                            <td>7</td>
                            <td><strong>70.8%</strong></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            
            <h3>Key Observations</h3>
            <ol>
                <li><strong>EACPS consistently wins on prompt following (CLIPScore)</strong> - 6/8 tasks. The multi-stage approach finds edits that better match the intended changes.</li>
                <li><strong>Aesthetic quality is improved</strong> - 6/8 tasks. Local refinement helps find visually pleasing variants.</li>
                <li><strong>LPIPS (preservation) is more variable</strong> - 5/8 tasks. Some edits require more change; the potential function sometimes trades preservation for edit accuracy.</li>
                <li><strong>Largest gains on complex edits</strong> - astronaut (+0.337 LPIPS improvement) and painter (+0.080) tasks showed the biggest improvements.</li>
            </ol>
        </section>
        
        <section id="codebase">
            <h2>7. Codebase Architecture</h2>
            
            <div class="file-tree">
qwen3-score/
├── src/                        <span class="comment"># Core source code</span>
│   ├── compare.py              <span class="comment"># Head-to-head comparison: TT-FLUX vs EACPS</span>
│   ├── batch_compare.py        <span class="comment"># Batch evaluation across multiple prompts</span>
│   ├── eacps.py                <span class="comment"># Core EACPS algorithm implementation</span>
│   ├── scorers.py              <span class="comment"># Multi-metric scoring (CLIP, LPIPS, Aesthetic)</span>
│   ├── benchmark.py            <span class="comment"># Benchmark suite runner</span>
│   └── setup_labelstudio.py    <span class="comment"># Generate Label Studio tasks for human eval</span>
├── data/                       <span class="comment"># Input data</span>
│   ├── bear.png                <span class="comment"># Test image</span>
│   ├── eval_prompts.jsonl      <span class="comment"># Benchmark prompts</span>
│   └── drawbench.jsonl         <span class="comment"># DrawBench prompts (from TT-FLUX)</span>
├── experiments/                <span class="comment"># Experimental results</span>
│   └── results_qwen_bear/      <span class="comment"># Bear benchmark results</span>
├── docs/                       <span class="comment"># Documentation</span>
│   ├── writeup.html            <span class="comment"># This document</span>
│   └── ttflux_reference.pdf    <span class="comment"># TT-FLUX paper</span>
└── requirements.txt            <span class="comment"># Dependencies</span>
            </div>
            
            <h3>Key Files</h3>
            
            <p><strong><code>src/eacps.py</code></strong> - Core EACPS implementation:</p>
            <ul>
                <li><code>build_pipeline()</code> - Load Qwen-Image-Edit model</li>
                <li><code>generate_edit()</code> - Single image generation</li>
                <li><code>compute_potential()</code> - Calculate potential score from metrics</li>
                <li><code>main()</code> - Full EACPS pipeline with two-stage search</li>
            </ul>
            
            <p><strong><code>src/compare.py</code></strong> - Head-to-head comparison:</p>
            <ul>
                <li><code>run_ttflux_style()</code> - Random search baseline</li>
                <li><code>run_eacps_style()</code> - EACPS multi-stage search</li>
                <li><code>compute_metrics()</code> - Evaluate with all metrics</li>
                <li>Outputs: JSON results + all candidate images</li>
            </ul>
            
            <p><strong><code>src/scorers.py</code></strong> - Evaluation metrics:</p>
            <ul>
                <li><code>EditScorer</code> - Main scoring class</li>
                <li><code>_clip_pf()</code> - CLIP prompt following</li>
                <li><code>_clip_cons()</code> - CLIP consistency</li>
                <li><code>_lpips_dist()</code> - LPIPS perceptual distance</li>
                <li><code>AestheticMLP</code> - LAION aesthetic predictor</li>
            </ul>
        </section>
        
        <section id="usage">
            <h2>8. Usage Guide</h2>
            
            <h3>Setup</h3>
<pre>
<code># Create and activate virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip3 install -r requirements.txt

# Verify CUDA is available
python3 -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"</code>
</pre>
            
            <h3>Run Single Comparison</h3>
<pre>
<code>python3 src/compare.py \
    --image data/bear.png \
    --prompt "Add a colorful art board and paintbrush in the bear's hands" \
    --output_dir experiments/my_test \
    --device cuda:0 \
    --num_samples 8 \
    --k_global 8 \
    --m_global 2 \
    --k_local 2 \
    --steps 50 \
    --cfg 5.0</code>
</pre>
            
            <h3>Output Structure</h3>
<pre>
<code>experiments/my_test/
├── input.png                 # Original image
├── ttflux_best.png           # Best TT-FLUX result
├── ttflux_candidate_*.png    # All TT-FLUX candidates
├── eacps_best.png            # Best EACPS result
├── eacps_candidate_*.png     # All EACPS candidates
└── comparison_results.json   # Full metrics comparison</code>
</pre>
            
            <h3>Run Batch Evaluation</h3>
<pre>
<code>python3 src/batch_compare.py \
    --prompts data/eval_prompts.jsonl \
    --output_dir experiments/batch_eval \
    --data_dir data \
    --devices cuda:0 \
    --num_samples 4 \
    --k_global 4 \
    --m_global 2 \
    --k_local 2</code>
</pre>
            
            <h3>Prompt File Format</h3>
<pre>
<code># data/eval_prompts.jsonl (one JSON object per line)
{"id": "task_001", "image": "bear.png", "prompt": "Add sunglasses to the bear"}
{"id": "task_002", "image": "bear.png", "prompt": "Make the bear wear a hat"}</code>
</pre>
            
            <h3>Custom Hyperparameters</h3>
            <p>Tune based on your compute budget and quality requirements:</p>
<pre>
<code># Fast mode (6 total generations)
--k_global 4 --m_global 2 --k_local 1

# Balanced mode (12 total generations, default)
--k_global 8 --m_global 2 --k_local 2

# High quality mode (24 total generations)
--k_global 16 --m_global 4 --k_local 2

# Adjust potential function weights
--alpha_cons 10.0   # Increase for more preservation
--beta_lpips 3.0    # Increase to penalize big changes
--gamma_realism 1.0 # Enable aesthetic score in ranking</code>
</pre>
        </section>
        
        <section id="contributing">
            <h2>9. Contributing</h2>
            
            <h3>Development Setup</h3>
<pre>
<code>git clone &lt;repo&gt;
cd qwen3-score
python3 -m venv .venv
source .venv/bin/activate
pip3 install -r requirements.txt

# Run a quick test
python3 src/compare.py \
    --image data/bear.png \
    --prompt "Make the bear smile" \
    --output_dir experiments/dev_test \
    --device cuda:0 \
    --num_samples 2 \
    --k_global 2 \
    --m_global 1 \
    --k_local 1</code>
</pre>
            
            <h3>Areas for Contribution</h3>
            <ol>
                <li><strong>Better potential functions:</strong> Learn weights from human preferences instead of hand-tuning</li>
                <li><strong>Smarter local search:</strong> Gradient-based refinement in latent space</li>
                <li><strong>Adaptive compute:</strong> Allocate more samples to difficult edits</li>
                <li><strong>More base models:</strong> Test with FLUX, SDXL, Stable Diffusion 3</li>
                <li><strong>Evaluation:</strong> Human preference studies, more diverse benchmarks</li>
            </ol>
            
            <h3>Code Style</h3>
            <ul>
                <li>Python 3.10+</li>
                <li>Type hints on all functions</li>
                <li>No emojis in code or comments</li>
                <li>Clear variable names (no single letters except loop indices)</li>
                <li>Minimal prints; use structured logging if needed</li>
                <li>All paths via CLI args or config, no hardcoding</li>
            </ul>
            
            <h3>Testing Changes</h3>
<pre>
<code># Verify your changes don't break the comparison
python3 src/compare.py \
    --image data/bear.png \
    --prompt "Add a hat" \
    --output_dir experiments/test_changes \
    --device cuda:0 \
    --num_samples 4 \
    --k_global 4 \
    --m_global 2 \
    --k_local 2

# Check the output JSON for expected structure
cat experiments/test_changes/comparison_results.json | python3 -m json.tool</code>
</pre>
        </section>
        
        <footer>
            <p><strong>EACPS:</strong> Efficient Adaptive Candidate-based Prompt Search for Image Editing</p>
            <p>Base Model: Qwen-Image-Edit | Metrics: TT-FLUX methodology</p>
            <p style="margin-top: 16px; color: #999;">Last updated: December 2024</p>
        </footer>
    </div>
</body>
</html>
